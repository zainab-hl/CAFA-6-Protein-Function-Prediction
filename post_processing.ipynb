{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T21:19:11.983688Z",
     "iopub.status.busy": "2026-01-31T21:19:11.981314Z",
     "iopub.status.idle": "2026-01-31T21:37:20.089172Z",
     "shell.execute_reply": "2026-01-31T21:37:20.088294Z",
     "shell.execute_reply.started": "2026-01-31T21:19:11.983581Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "\n",
    "\n",
    "DATA_ROOT = \"/kaggle/input/cafa-6-protein-function-prediction\"\n",
    "GO_OBO = os.path.join(DATA_ROOT, \"Train\", \"go-basic.obo\")\n",
    "IA_TSV = os.path.join(DATA_ROOT, \"IA.tsv\")\n",
    "TRAIN_TAXONOMY = os.path.join(DATA_ROOT, \"Train\", \"train_taxonomy.tsv\")\n",
    "TEST_FASTA = os.path.join(DATA_ROOT, \"Test\", \"testsuperset.fasta\")\n",
    "\n",
    "GOA_PRED_FILE = os.path.join(\"/kaggle/input/protein-go-annotations\", \"goa_uniprot_ver228.tsv\")\n",
    "ESM_PRED_FILE = os.path.join(\"/kaggle/input/submission\", \"submission.tsv\") ### my previous ESM predictions\n",
    "OUTPUT_FILE = \"submission.tsv\"\n",
    "\n",
    "\n",
    "CONFIG = {\n",
    "    'TOP_K': 200,               \n",
    "    'MIN_SCORE': 0.001,        \n",
    "    'MAX_SCORE': 0.97,\n",
    "    'NEG_PROP_ALPHA': 0.7,\n",
    "    'SCALING_POWER': 0.8,\n",
    "    'USE_IA': True,\n",
    "    'USE_TAXONOMY': True,\n",
    "    'IA_BOOST_FACTOR': 1.2,     # idk, im just changing them\n",
    "    'TAXONOMY_BOOST': 1.15,     \n",
    "    'WEIGHT_GOA': 0.55,\n",
    "    'WEIGHT_ESM': 0.45,\n",
    "}\n",
    "\n",
    "\n",
    "print(\"Parsing GO ontology...\")\n",
    "term_parents = defaultdict(set)\n",
    "\n",
    "with open(GO_OBO, 'r') as f:\n",
    "    cur_id = None\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith('id: '):\n",
    "            cur_id = line.split('id: ')[1].strip()\n",
    "        elif line.startswith('is_a: ') and cur_id:\n",
    "            term_parents[cur_id].add(line.split()[1].strip())\n",
    "        elif line.startswith('relationship: part_of ') and cur_id:\n",
    "            term_parents[cur_id].add(line.split()[2].strip())\n",
    "\n",
    "ROOTS = {'GO:0003674', 'GO:0008150', 'GO:0005575'}\n",
    "\n",
    "ancestors_map = {}\n",
    "def get_ancestors(term):\n",
    "    if term in ancestors_map:\n",
    "        return ancestors_map[term]\n",
    "    parents = term_parents.get(term, set())\n",
    "    all_anc = set(parents)\n",
    "    for p in parents:\n",
    "        all_anc |= get_ancestors(p)\n",
    "    ancestors_map[term] = all_anc\n",
    "    return all_anc\n",
    "\n",
    "# Precompute ancestors for efficiency\n",
    "for term in tqdm(term_parents.keys(), desc=\"Precomputing ancestors\"):\n",
    "    get_ancestors(term)\n",
    "\n",
    "print(f\"Parsed GO ontology with {len(ancestors_map)} terms\")\n",
    "\n",
    "ia_weights = {}\n",
    "if CONFIG['USE_IA']:\n",
    "    print(\"\\nLoading IA weights...\")\n",
    "    ia_df = pd.read_csv(IA_TSV, sep='\\t', header=None, \n",
    "                       names=['go_term', 'ia_weight'])\n",
    "    \n",
    "    # Normalize to 0-1 range, idk if it's helpful\n",
    "    min_ia = ia_df['ia_weight'].min()\n",
    "    max_ia = ia_df['ia_weight'].max()\n",
    "    if max_ia > min_ia:\n",
    "        ia_df['norm_weight'] = (ia_df['ia_weight'] - min_ia) / (max_ia - min_ia)\n",
    "    else:\n",
    "        ia_df['norm_weight'] = 1.0\n",
    "    \n",
    "    ia_weights = dict(zip(ia_df['go_term'], ia_df['norm_weight']))\n",
    "    print(f\"Loaded IA weights for {len(ia_weights)} terms\")\n",
    "\n",
    "train_tax_dict = {}\n",
    "test_tax_dict = {}\n",
    "taxon_to_go = {}\n",
    "\n",
    "if CONFIG['USE_TAXONOMY']:\n",
    "    print(\"\\nLoading taxonomy information...\")\n",
    "    \n",
    "    train_tax = pd.read_csv(TRAIN_TAXONOMY, sep='\\t', \n",
    "                           names=['protein_id', 'taxon_id'])\n",
    "    train_tax_dict = dict(zip(train_tax['protein_id'], train_tax['taxon_id']))\n",
    "    \n",
    "    test_tax_dict = {}\n",
    "    if os.path.exists(TEST_FASTA):\n",
    "        with open(TEST_FASTA, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('>'):\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 2:\n",
    "                        protein_id = parts[0][1:]\n",
    "                        for part in parts[1:]:\n",
    "                            if part.isdigit() and 4 <= len(part) <= 10:\n",
    "                                test_tax_dict[protein_id] = int(part)\n",
    "                                break\n",
    "        \n",
    "        print(f\"Extracted taxonomy for {len(test_tax_dict)} test proteins\")\n",
    "    \n",
    "    train_terms_path = os.path.join(DATA_ROOT, \"Train\", \"train_terms.tsv\")\n",
    "    if os.path.exists(train_terms_path):\n",
    "        train_terms = pd.read_csv(train_terms_path, sep='\\t', \n",
    "                                 names=['protein_id', 'go_term', 'aspect'])\n",
    "        \n",
    "        train_merged = train_terms.merge(\n",
    "            pd.DataFrame(list(train_tax_dict.items()), columns=['protein_id', 'taxon_id']),\n",
    "            on='protein_id', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        for _, row in train_merged.iterrows():\n",
    "            if pd.notna(row['taxon_id']):\n",
    "                taxon_id = int(row['taxon_id'])\n",
    "                if taxon_id not in taxon_to_go:\n",
    "                    taxon_to_go[taxon_id] = set()\n",
    "                taxon_to_go[taxon_id].add(row['go_term'])\n",
    "        \n",
    "        print(f\"Mapped {len(taxon_to_go)} taxa to GO terms\")\n",
    "\n",
    "\n",
    "def load_predictions(filepath, is_goa=False):\n",
    "    \"\"\"Load predictions with chunking\"\"\"\n",
    "    data = defaultdict(dict)\n",
    "    \n",
    "    if is_goa:\n",
    "        try:\n",
    "            for chunk in pd.read_csv(filepath, sep='\\t', header=0, \n",
    "                                    chunksize=100000):\n",
    "                for _, row in chunk.iterrows():\n",
    "                    try:\n",
    "                        pid = str(row.iloc[0]).strip()\n",
    "                        go = str(row.iloc[1]).strip()\n",
    "                        score = 1.0\n",
    "                        if len(row) >= 3:\n",
    "                            try:\n",
    "                                score = float(row.iloc[2])\n",
    "                            except (ValueError, TypeError):\n",
    "                                score = 1.0\n",
    "                        data[pid][go] = max(data[pid].get(go, 0), score)\n",
    "                    except:\n",
    "                        continue\n",
    "        except:\n",
    "            with open(filepath, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                start_idx = 1 if 'qualifier' in lines[0].lower() else 0\n",
    "                for line in tqdm(lines[start_idx:], desc=f\"Loading GOA\"):\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) >= 2:\n",
    "                        pid, go = parts[0], parts[1]\n",
    "                        score = 1.0\n",
    "                        if len(parts) >= 3:\n",
    "                            try:\n",
    "                                score = float(parts[2])\n",
    "                            except:\n",
    "                                score = 1.0\n",
    "                        data[pid][go] = max(data[pid].get(go, 0), score)\n",
    "    else:\n",
    "        # ESM predictions\n",
    "        try:\n",
    "            for chunk in pd.read_csv(filepath, sep='\\t', header=None,\n",
    "                                    names=['protein', 'go', 'score'],\n",
    "                                    chunksize=100000):\n",
    "                for _, row in chunk.iterrows():\n",
    "                    pid, go, score = str(row['protein']), str(row['go']), float(row['score'])\n",
    "                    data[pid][go] = max(data[pid].get(go, 0), score)\n",
    "        except:\n",
    "            with open(filepath, 'r') as f:\n",
    "                for line in tqdm(f, desc=\"Loading ESM\"):\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) >= 3:\n",
    "                        pid, go, score = parts[0], parts[1], float(parts[2])\n",
    "                        data[pid][go] = max(data[pid].get(go, 0), score)\n",
    "    \n",
    "    return data\n",
    "\n",
    "print(\"\\nLoading predictions...\")\n",
    "goa_preds = load_predictions(GOA_PRED_FILE, is_goa=True)\n",
    "esm_preds = load_predictions(ESM_PRED_FILE, is_goa=False)\n",
    "\n",
    "print(f\"GOA proteins: {len(goa_preds):,}\")\n",
    "print(f\"ESM2 proteins: {len(esm_preds):,}\")\n",
    "\n",
    "\n",
    "print(\"\\nCreating enhanced ensemble...\")\n",
    "esm_proteins = set(esm_preds.keys())\n",
    "goa_proteins = set(goa_preds.keys())\n",
    "all_proteins = esm_proteins & goa_proteins  \n",
    "\n",
    "ensemble = defaultdict(dict)\n",
    "\n",
    "for pid in tqdm(all_proteins, desc=\"Ensemble with IA\"):\n",
    "    goa = goa_preds.get(pid, {})\n",
    "    esm = esm_preds.get(pid, {})\n",
    "    all_terms = set(goa.keys()) | set(esm.keys())\n",
    "    \n",
    "    for term in all_terms:\n",
    "        s_goa = goa.get(term, 0)\n",
    "        s_esm = esm.get(term, 0)\n",
    "        \n",
    "        if s_goa > 0 and s_esm > 0:\n",
    "            base_score = CONFIG['WEIGHT_GOA'] * s_goa + CONFIG['WEIGHT_ESM'] * s_esm\n",
    "        elif s_goa > 0:\n",
    "            base_score = s_goa\n",
    "        else:\n",
    "            base_score = s_esm\n",
    "        \n",
    "        if CONFIG['USE_IA']:\n",
    "            ia_weight = ia_weights.get(term, 0.5)\n",
    "            boost = 1.0 + (CONFIG['IA_BOOST_FACTOR'] - 1.0) * ia_weight\n",
    "            base_score = min(base_score * boost, CONFIG['MAX_SCORE'])\n",
    "        \n",
    "        ensemble[pid][term] = base_score\n",
    "\n",
    "print(f\"Ensemble created: {len(ensemble):,} proteins\")\n",
    "\n",
    "\n",
    "def enhanced_process_protein(protein_id, scores_dict):\n",
    "    \"\"\"Enhanced version of process_protein with IA and taxonomy\"\"\"\n",
    "    updated = scores_dict.copy()\n",
    "    \n",
    "    if CONFIG['USE_TAXONOMY']:\n",
    "        test_taxon = test_tax_dict.get(protein_id)\n",
    "        if test_taxon and test_taxon in taxon_to_go:\n",
    "            common_terms = taxon_to_go[test_taxon]\n",
    "            for term in list(updated.keys()):\n",
    "                if term in common_terms:\n",
    "                    updated[term] = min(updated[term] * CONFIG['TAXONOMY_BOOST'], CONFIG['MAX_SCORE'])\n",
    "    \n",
    "    # Positive propagation: ensure parent >= child, explanation in readme\n",
    "    for term, score in scores_dict.items():\n",
    "        for anc in get_ancestors(term):\n",
    "            current = updated.get(anc, 0)\n",
    "            if score > current:\n",
    "                updated[anc] = score\n",
    "    \n",
    "    # Negative propagation: child â‰¤ parent\n",
    "    for term in list(updated.keys()):\n",
    "        if term in ROOTS:\n",
    "            continue\n",
    "        ancs = get_ancestors(term)\n",
    "        if ancs:\n",
    "            anc_scores = [updated.get(a, 0) for a in ancs if a in updated]\n",
    "            if anc_scores and min(anc_scores) < updated[term]:\n",
    "                alpha = CONFIG['NEG_PROP_ALPHA']\n",
    "                updated[term] = alpha * min(anc_scores) + (1 - alpha) * updated[term]\n",
    "    \n",
    "    # Power scaling\n",
    "    non_root = [s for t, s in updated.items() if t not in ROOTS]\n",
    "    if non_root:\n",
    "        max_val = max(non_root)\n",
    "        if 0 < max_val < CONFIG['MAX_SCORE']:\n",
    "            for t in updated:\n",
    "                if t not in ROOTS:\n",
    "                    new_score = np.power(updated[t] / max_val, CONFIG['SCALING_POWER']) * CONFIG['MAX_SCORE']\n",
    "                    updated[t] = min(CONFIG['MAX_SCORE'], new_score)\n",
    "    \n",
    "    # Force roots\n",
    "    for r in ROOTS:\n",
    "        updated[r] = 1.0\n",
    "    \n",
    "    return updated\n",
    "\n",
    "\n",
    "print(f\"\\nApplying enhanced propagation (TOP_K={CONFIG['TOP_K']})...\")\n",
    "final_rows = []\n",
    "\n",
    "for pid, scores in tqdm(ensemble.items(), desc=\"Processing proteins\"):\n",
    "    updated = enhanced_process_protein(pid, scores)\n",
    "    \n",
    "    sorted_terms = sorted(updated.items(), key=lambda x: -x[1])\n",
    "    \n",
    "    kept_terms = []\n",
    "    for term, score in sorted_terms:\n",
    "        if score >= CONFIG['MIN_SCORE']:\n",
    "            kept_terms.append((term, score))\n",
    "            if len(kept_terms) >= CONFIG['TOP_K']:\n",
    "                break\n",
    "    \n",
    "    for term, score in kept_terms:\n",
    "        final_rows.append(f\"{pid}\\t{term}\\t{score:.6f}\")\n",
    "\n",
    "\n",
    "print(f\"\\nSaving {len(final_rows):,} predictions...\")\n",
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    for line in final_rows:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "size_mb = os.path.getsize(OUTPUT_FILE) / (1024 * 1024)\n",
    "print(f\"Submission file saved: {OUTPUT_FILE}\")\n",
    "print(f\"Size: {size_mb:.1f} MB, Predictions: {len(final_rows):,}\")\n",
    "print(f\"Average predictions per protein: {len(final_rows)/len(ensemble):.2f}\")\n",
    "\n",
    "## summary\n",
    "summary_file = \"enhanced_top200_summary.txt\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(\"=== ENHANCED ENSEMBLE WITH TOP_K=200 ===\\n\")\n",
    "    f.write(f\"GO ontology terms: {len(ancestors_map)}\\n\")\n",
    "    f.write(f\"IA weights loaded: {CONFIG['USE_IA']}\\n\")\n",
    "    f.write(f\"Taxonomy data loaded: {CONFIG['USE_TAXONOMY']}\\n\")\n",
    "    f.write(f\"GOA proteins: {len(goa_preds)}\\n\")\n",
    "    f.write(f\"ESM proteins: {len(esm_preds)}\\n\")\n",
    "    f.write(f\"Ensemble proteins (intersection): {len(ensemble)}\\n\")\n",
    "    f.write(f\"Final predictions: {len(final_rows)}\\n\")\n",
    "    f.write(f\"Average predictions per protein: {len(final_rows)/len(ensemble):.2f}\\n\")\n",
    "    f.write(f\"File size: {size_mb:.1f} MB\\n\")\n",
    "    f.write(f\"\\nConfiguration:\\n\")\n",
    "    for key, value in CONFIG.items():\n",
    "        f.write(f\"  {key}: {value}\\n\")\n",
    "\n",
    "print(f\"\\nSummary saved to: {summary_file}\")\n",
    "\n",
    "### preview\n",
    "print(\"\\nSample predictions (first 10):\")\n",
    "for i, line in enumerate(final_rows[:10]):\n",
    "    print(f\"  {line.strip()}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14875579,
     "sourceId": 116062,
     "sourceType": "competition"
    },
    {
     "datasetId": 9045748,
     "sourceId": 14187577,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9362153,
     "sourceId": 14691421,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
